{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertModel, AutoTokenizer, RobertaModel, RobertaTokenizerFast\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, normalize\n",
    "import torch, os, random\n",
    "from Sophia.sophia import SophiaG \n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from graph_aug import mask_nodes, mask_edges, permute_edges, drop_nodes, subgraph\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "from ogb.utils import smiles2graph\n",
    "from dualgraph.mol import smiles2graphwithface, simles2graphwithface_with_mask\n",
    "from dualgraph.gnn import GNN, GNN2, GIN_node_Virtual, GNNwithvn\n",
    "\n",
    "from torch_geometric.data import Dataset, InMemoryDataset\n",
    "from dualgraph.dataset import DGData\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "seed_everything(2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv', index_col=None)\n",
    "test = pd.read_csv('data/test.csv', index_col=None)\n",
    "\n",
    "duplicate_smile = data['SMILES'].value_counts().reset_index().query('count>1')['SMILES'].values\n",
    "no_duplicate = data.drop_duplicates('SMILES').reset_index(drop=True)\n",
    "\n",
    "duplicate = data[data['SMILES'].isin(duplicate_smile)].reset_index(drop=True)\n",
    "duplicate = duplicate.groupby('SMILES')[['MLM', 'HLM']].max().reset_index()\n",
    "\n",
    "no_duplicate.loc[ no_duplicate['SMILES'].isin(duplicate['SMILES']), 'MLM'] = duplicate['MLM'].values\n",
    "no_duplicate.loc[ no_duplicate['SMILES'].isin(duplicate['SMILES']), 'HLM'] = duplicate['HLM'].values\n",
    "data = no_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['AlogP'].isna(), 'AlogP'] = data['LogD']\n",
    "test.loc[test['AlogP'].isna(), 'AlogP'] = test['LogD']\n",
    "\n",
    "# data.loc[data['MLM'] > 100, 'MLM'] = float(100)\n",
    "# data.loc[data['HLM'] > 100, 'HLM'] = float(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_target = np.log1p(data['MLM'].values)\n",
    "norm_pp_target = (pp_target - pp_target.mean()) / (pp_target.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['MLM'] = 0\n",
    "test['HLM'] = 0\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(InMemoryDataset):\n",
    "    def __init__(self, root='dataset_path', transform=None, pre_transform=None, df=None, target_type='MLM', mode='train'):\n",
    "        self.df = df\n",
    "        self.target_type = target_type\n",
    "        self.mode = mode\n",
    "        super().__init__(root, transform, pre_transform, df)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):        \n",
    "        return [f'raw_{i+1}.pt' for i in range(self.df.shape[0])]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{i+1}.pt' for i in range(self.df.shape[0])]        \n",
    "\n",
    "    def len(self):\n",
    "        return len(self.graph_list)\n",
    "\n",
    "    def get(self, idx):        \n",
    "        if self.mode=='train' and random.random() > 10:\n",
    "            data = DGData()\n",
    "            smiles = self.smiles_list[idx]\n",
    "            smiles = sme.randomize_smiles(smiles)                                \n",
    "\n",
    "            targets = self.targets_list[idx]\n",
    "\n",
    "            graph = smiles2graphwithface(smiles)\n",
    "\n",
    "            data.__num_nodes__ = int(graph[\"num_nodes\"])\n",
    "            data.edge_index = torch.from_numpy(graph[\"edge_index\"]).to(torch.int64)\n",
    "            data.edge_attr = torch.from_numpy(graph[\"edge_feat\"]).to(torch.int64)\n",
    "            data.x = torch.from_numpy(graph[\"node_feat\"]).to(torch.int64)\n",
    "            data.y = torch.Tensor([targets])\n",
    "\n",
    "            data.ring_mask = torch.from_numpy(graph[\"ring_mask\"]).to(torch.bool)\n",
    "            data.ring_index = torch.from_numpy(graph[\"ring_index\"]).to(torch.int64)\n",
    "            data.nf_node = torch.from_numpy(graph[\"nf_node\"]).to(torch.int64)\n",
    "            data.nf_ring = torch.from_numpy(graph[\"nf_ring\"]).to(torch.int64)\n",
    "            data.num_rings = int(graph[\"num_rings\"])\n",
    "            data.n_edges = int(graph[\"n_edges\"])\n",
    "            data.n_nodes = int(graph[\"n_nodes\"])\n",
    "            data.n_nfs = int(graph[\"n_nfs\"])\n",
    "            data.tabular = torch.from_numpy(self.tabular_list[idx])\n",
    "\n",
    "            return data\n",
    "\n",
    "        else:\n",
    "            # if self.mode=='train' and random.random() < 0.5:\n",
    "            #     return mask_nodes(self.graph_list[idx], 0.1)\n",
    "            return self.graph_list[idx]\n",
    "\n",
    "\n",
    "    def process(self):        \n",
    "        smiles_list = self.df[\"SMILES\"].values\n",
    "        targets_list = self.df[self.target_type].values\n",
    "        tabular_list = self.df[['AlogP', 'Molecular_Weight', 'Num_H_Acceptors', 'Num_H_Donors', 'Num_RotatableBonds', 'LogD', 'Molecular_PolarSurfaceArea']].values.astype('float32')\n",
    "\n",
    "        data_list = []\n",
    "        for i in tqdm(range(len(smiles_list))):            \n",
    "            data = DGData()\n",
    "            smiles = smiles_list[i]\n",
    "            targets = targets_list[i]\n",
    "            graph = smiles2graphwithface(smiles)\n",
    "\n",
    "            data.__num_nodes__ = int(graph[\"num_nodes\"])\n",
    "            data.edge_index = torch.from_numpy(graph[\"edge_index\"]).to(torch.int64)\n",
    "            data.edge_attr = torch.from_numpy(graph[\"edge_feat\"]).to(torch.int64)\n",
    "            data.x = torch.from_numpy(graph[\"node_feat\"]).to(torch.int64)\n",
    "            data.y = torch.Tensor([targets])\n",
    "\n",
    "            data.ring_mask = torch.from_numpy(graph[\"ring_mask\"]).to(torch.bool)\n",
    "            data.ring_index = torch.from_numpy(graph[\"ring_index\"]).to(torch.int64)\n",
    "            data.nf_node = torch.from_numpy(graph[\"nf_node\"]).to(torch.int64)\n",
    "            data.nf_ring = torch.from_numpy(graph[\"nf_ring\"]).to(torch.int64)\n",
    "            data.num_rings = int(graph[\"num_rings\"])\n",
    "            data.n_edges = int(graph[\"n_edges\"])\n",
    "            data.n_nodes = int(graph[\"n_nodes\"])\n",
    "            data.n_nfs = int(graph[\"n_nfs\"])        \n",
    "            data.tabular = torch.from_numpy(tabular_list[i])\n",
    "\n",
    "            data_list.append(data)\n",
    "        self.smiles_list = smiles_list  \n",
    "        self.graph_list = data_list\n",
    "        self.targets_list = targets_list\n",
    "        self.tabular_list = tabular_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_score(y_true, y_pred):\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1)[:, None]\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1)[:, None]\n",
    "    cov_tp = torch.sum(y_true_centered * y_pred_centered, dim=1) / (y_true.shape[1] - 1)\n",
    "    var_t = torch.sum(y_true_centered ** 2, dim=1) / (y_true.shape[1] - 1)\n",
    "    var_p = torch.sum(y_pred_centered ** 2, dim=1) / (y_true.shape[1] - 1)\n",
    "    return cov_tp / torch.sqrt(var_t * var_p)\n",
    "\n",
    "\n",
    "def correlation_loss(pred, target):\n",
    "    return -torch.mean(correlation_score(target.unsqueeze(0), pred.unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MedModel, self).__init__()\n",
    "        self.ddi = True\n",
    "        self.gnn = GNN2(\n",
    "                        mlp_hidden_size = 512,\n",
    "                        mlp_layers = 2,\n",
    "                        latent_size = 128,\n",
    "                        use_layer_norm = False,\n",
    "                        use_face=True,\n",
    "                        # residual = True,\n",
    "                        ddi=self.ddi,\n",
    "                        dropedge_rate = 0.1,\n",
    "                        dropnode_rate = 0.1,\n",
    "                        dropout = 0.1,\n",
    "                        dropnet = 0.1,\n",
    "                        global_reducer = \"sum\",\n",
    "                        node_reducer = \"sum\",\n",
    "                        face_reducer = \"sum\",\n",
    "                        graph_pooling = \"sum\",\n",
    "                        # global_attn = True,\n",
    "                        node_attn = True,\n",
    "                        face_attn = True\n",
    "                        # use_bn=True\n",
    "                        )\n",
    "        state_dict=  torch.load('ckpt/ognn_pretrain_best.pt', map_location='cpu')        \n",
    "        self.gnn.load_state_dict(state_dict)#, strict=False)\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "                    nn.LayerNorm(128),\n",
    "                    nn.Linear(128, 128,),\n",
    "                    nn.BatchNorm1d(128),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128, 1),\n",
    "                    )\n",
    "\n",
    "        # self.fc1[1].weight.data.normal_(mean=0.0, std=0.01)\n",
    "        self.fc1[-1].weight.data.normal_(mean=0.0, std=0.01)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        mol = self.gnn(batch)\n",
    "        out1 = self.fc1(mol).squeeze(1)# .sigmoid() * 100\n",
    "        return out1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_mse_loss(preds, targets):\n",
    "    p_mean, p_var = preds.mean(), preds.var()\n",
    "    t_mean, t_var = targets.mean(), targets.var()\n",
    "    norm_preds = (preds - p_mean) / (p_var + 1.e-6)**.5\n",
    "    norm_targets = (targets - t_mean) / (t_var + 1.e-6)**.5\n",
    "    loss = (norm_preds - norm_targets) ** 2\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ksplit = KFold(n_splits=5, shuffle=True, random_state=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x-target_mean) / target_std\n",
    "\n",
    "def denorm(x):\n",
    "    return x * target_std + target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_chem_df = pd.read_csv('data2/train-aug-MLM-0906.csv.csv')\n",
    "hlm_chem_df = pd.read_csv('data2/train-aug-HLM-0906.csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, (t_idx, v_idx) in enumerate(ksplit.split(range(data.shape[0]))):\n",
    "#     train, valid = data.loc[t_idx].reset_index(drop=True), data.loc[v_idx].reset_index(drop=True)\n",
    "    \n",
    "#     target_mean = train['MLM'].mean()\n",
    "#     target_std = train['MLM'].std()\n",
    "\n",
    "#     train = pd.concat([train, mlm_chem_df]).reset_index(drop=True)\n",
    "\n",
    "#     train_dataset = CustomDataset(df = train, mode='train')\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers = 8)\n",
    "\n",
    "#     valid_dataset = CustomDataset(df = valid, mode='test')\n",
    "#     valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers = 8)\n",
    "\n",
    "#     model = MedModel().to(device)\n",
    "#     # model.load_state_dict(torch.load('best_gnn_semi_MLM.pt'))\n",
    "#     hub_loss = nn.HuberLoss()\n",
    "#     mse_loss = nn.MSELoss()\n",
    "#     optim = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "#     ema = ExponentialMovingAverage(model.parameters(), decay=0.995)\n",
    "#     scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=10, T_mult=1, verbose=False)\n",
    "#     # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=80, verbose=False)\n",
    "#     best_val_loss = 1e6\n",
    "\n",
    "#     for epoch in range(50):\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         for batch in train_loader:\n",
    "#             batch = batch.to(device)\n",
    "            \n",
    "#             preds = model(batch)\n",
    "#             targets = batch.y.to(device)\n",
    "#             targets = norm(targets)\n",
    "\n",
    "#             loss = mse_loss(preds, targets) * 0.8 + correlation_loss(preds, targets) * 0.2\n",
    "\n",
    "#             optim.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optim.step()\n",
    "#             ema.update()\n",
    "            \n",
    "#             train_loss += loss.cpu().item()\n",
    "        \n",
    "#         model.eval()\n",
    "#         valid_preds = []\n",
    "#         valid_label = []\n",
    "            \n",
    "#         for batch in valid_loader:\n",
    "#             batch = batch.to(device)\n",
    "#             with torch.no_grad():\n",
    "#                 preds = model(batch)                \n",
    "#                 targets = batch.y.to(device)                            \n",
    "#                 preds = denorm(preds) \n",
    "                \n",
    "#                 valid_label += targets.cpu().tolist()\n",
    "#                 valid_preds += preds.cpu().tolist()\n",
    "        \n",
    "#         val_loss = mean_squared_error(valid_preds, valid_label) ** (1/2)\n",
    "\n",
    "#         if val_loss < best_val_loss:\n",
    "#             best_val_loss = val_loss\n",
    "#             # np.save(f'ckpt_fold/best_gnn_val_preds_{k}.npy', np.array(valid_preds))\n",
    "#             # np.save(f'ckpt_fold/val_label_{k}.npy', np.array(valid_label))\n",
    "#             # torch.save(model.state_dict(), f'ckpt_fold/best_gnn_{train_dataset.target_type}_{k}.pt')\n",
    "#         scheduler.step()\n",
    "#         print(f'EPOCH : {epoch} | T_LOSS : {train_loss / len(train_loader):.4f} | MLM_RMSE : {val_loss:.2f} | BEST : {best_val_loss:.2f}')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pjh/anaconda3/envs/crash/lib/python3.8/site-packages/torch_geometric/data/dataset.py:217: UserWarning: The `pre_filter` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-fitering technique, make sure to delete '{self.processed_dir}' first\n",
      "  warnings.warn(\n",
      "Processing...\n",
      "100%|██████████| 4373/4373 [00:04<00:00, 1086.21it/s]\n",
      "Done!\n",
      "/home/pjh/anaconda3/envs/crash/lib/python3.8/site-packages/torch_geometric/data/dataset.py:217: UserWarning: The `pre_filter` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-fitering technique, make sure to delete '{self.processed_dir}' first\n",
      "  warnings.warn(\n",
      "Processing...\n",
      "100%|██████████| 695/695 [00:00<00:00, 1163.90it/s]\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 0 | T_LOSS : 3246.8391 | HLM_RMSE : 62.92 | BEST : 62.92\n"
     ]
    }
   ],
   "source": [
    "for k, (t_idx, v_idx) in enumerate(ksplit.split(range(data.shape[0]))):\n",
    "    train, valid = data.loc[t_idx].reset_index(drop=True), data.loc[v_idx].reset_index(drop=True)\n",
    "\n",
    "    target_mean = train['HLM'].mean()\n",
    "    target_std = train['HLM'].std()\n",
    "\n",
    "    train = pd.concat([train, hlm_chem_df]).reset_index(drop=True)\n",
    "\n",
    "    train_dataset = CustomDataset(df = train, mode='train', target_type='HLM')\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers = 8)\n",
    "\n",
    "    valid_dataset = CustomDataset(df = valid, mode='test', target_type='HLM')\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=128, shuffle=False, num_workers = 8)\n",
    "    \n",
    "\n",
    "    model = MedModel().to(device)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    ema = ExponentialMovingAverage(model.parameters(), decay=0.995)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optim, T_0=10, T_mult=1, verbose=False)\n",
    "    best_val_loss = 1e6\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            \n",
    "            preds = model(batch)\n",
    "            targets = batch.y.to(device)\n",
    "            targets = norm(targets)\n",
    "            \n",
    "            loss = mse_loss(preds, targets) * 0.8 + correlation_loss(preds, targets) * 0.2\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            ema.update()\n",
    "            \n",
    "            train_loss += loss.cpu().item()\n",
    "        \n",
    "        model.eval()\n",
    "        valid_preds = []\n",
    "        valid_label = []\n",
    "            \n",
    "        for batch in valid_loader:\n",
    "            batch = batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                preds = model(batch)                \n",
    "                targets = batch.y.to(device)                            \n",
    "                preds = denorm(preds) \n",
    "\n",
    "                valid_label += targets.cpu().tolist()\n",
    "                valid_preds += preds.cpu().tolist()\n",
    "        \n",
    "        val_loss = mean_squared_error(valid_preds, valid_label) ** (1/2)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            np.save(f'ckpt_fold/best_gnn_val_preds_HLM_{k}.npy', np.array(valid_preds))\n",
    "            np.save(f'ckpt_fold/val_label_HLM_{k}.npy', np.array(valid_label))\n",
    "            torch.save(model.state_dict(), f'ckpt_fold/best_gnn_{train_dataset.target_type}_{k}.pt')\n",
    "        scheduler.step()\n",
    "        print(f'EPOCH : {epoch} | T_LOSS : {train_loss / len(train_loader):.4f} | HLM_RMSE : {val_loss:.2f} | BEST : {best_val_loss:.2f}')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = CustomDataset(df = test, mode='test', target_type='MLM')\n",
    "# test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ognn_MLM = []\n",
    "# ognn_HLM = []\n",
    "\n",
    "# for k in tqdm(range(5)):\n",
    "#     model = MedModel().to(device)\n",
    "#     model.load_state_dict(torch.load(f'ckpt_fold/best_gnn_MLM_{k}.pt'))\n",
    "#     model.eval()\n",
    "#     preds = []\n",
    "#     for batch in test_loader:\n",
    "#         with torch.no_grad():            \n",
    "#             preds += model.fc1(model.gnn(batch.to(device))).cpu().tolist()\n",
    "#     ognn_MLM.append(preds)\n",
    "\n",
    "# for k in tqdm(range(5)):\n",
    "#     model = MedModel().to(device)\n",
    "#     model.load_state_dict(torch.load(f'ckpt_fold/best_gnn_HLM_{k}.pt'))\n",
    "#     model.eval()\n",
    "#     preds = []\n",
    "\n",
    "#     for batch in test_loader:\n",
    "#         with torch.no_grad():            \n",
    "#             preds += model.fc1(model.gnn(batch.to(device))).cpu().tolist()  \n",
    "#     ognn_HLM.append(preds)\n",
    "\n",
    "# ognn_MLM = torch.tensor(ognn_MLM)\n",
    "# ognn_HLM = torch.tensor(ognn_HLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
